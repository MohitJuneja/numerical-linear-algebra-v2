---

title: 1. Why are we here?


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/01-Why-are-we-here.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/01-Why-are-we-here.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can read an overview of this Numerical Linear Algebra course in <a href="http://www.fast.ai/2017/07/17/num-lin-alg/">this blog post</a>.  The course was originally taught in the <a href="https://www.usfca.edu/arts-sciences/graduate-programs/analytics">University of San Francisco MS in Analytics</a> graduate program.  Course lecture videos are <a href="https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY">available on YouTube</a> (note that the notebook numbers and video numbers do not line up, since some notebooks took longer than 1 video to cover).</p>
<p>You can ask questions about the course on <a href="http://forums.fast.ai/c/lin-alg">our fast.ai forums</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Note: Future lessons have a lot more code than this one</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Why-study-Numerical-Linear-Algebra?">Why study Numerical Linear Algebra?<a class="anchor-link" href="#Why-study-Numerical-Linear-Algebra?"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Key Question of this course</strong>: How can we do matrix computations with acceptable speed and acceptable accuracy?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A list of the <a href="http://www.cs.fsu.edu/~lacher/courses/COT4401/notes/cise_v2_i1/index.html">Top 10 Algorithms</a> of science and engineering during the 20th century includes: the <strong>matrix decompositions</strong> approach to linear algebra. It also includes the QR algorithm, which we'll cover, and Krylov iterative methods which we'll see an example of. (See here for <a href="https://nickhigham.wordpress.com/2016/03/29/the-top-10-algorithms-in-applied-mathematics/">another take</a>)</p>
<p>{% include image.html alt="" style="width: 50%" file="/numerical_linear_algebra/images/top10.png" %}
(source: <a href="http://www.cs.fsu.edu/~lacher/courses/COT4401/notes/cise_v2_i1/guest.pdf">Top 10 Algorithms</a>)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are 4 things to keep in mind when choosing or designing an algorithm for matrix computations:</p>
<ul>
<li>Memory Use</li>
<li>Speed</li>
<li>Accuracy</li>
<li>Scalability/Parallelization</li>
</ul>
<p>Often there will be trade-offs between these categories.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Motivation">Motivation<a class="anchor-link" href="#Motivation"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Matrices are everywhere-- anything that can be put in an Excel spreadsheet is a matrix, and language and pictures can be represented as matrices as well.  Knowing what options there are for matrix algorithms, and how to navigate compromises, can make enormous differences to your solutions. For instance, an approximate matrix computation can often be thousands of times faster than an exact one.</p>
<p>It's not just about knowing the contents of existing libraries, but knowing how they work too. That's because often you can make variations to an algorithm that aren't supported by your library, giving you the performance or accuracy that you need. In addition, this field is moving very quickly at the moment, particularly in areas related to <strong>deep learning</strong>, <strong>recommendation systems</strong>, <strong>approximate algorithms</strong>, and <strong>graph analytics</strong>, so you'll often find there's recent results that could make big differences in your project, but aren't in your library.</p>
<p>Knowing how the algorithms really work helps to both debug and accelerate your solution.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Matrix-Computations">Matrix Computations<a class="anchor-link" href="#Matrix-Computations"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are two key types of matrix computation, which get combined in many different ways. These are:</p>
<ul>
<li>Matrix and tensor products</li>
<li>Matrix decompositions</li>
</ul>
<p>So basically we're going to be combining matrices, and pulling them apart again!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Matrix-and-Tensor-Products">Matrix and Tensor Products<a class="anchor-link" href="#Matrix-and-Tensor-Products"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Matrix-Vector-Products:">Matrix-Vector Products:<a class="anchor-link" href="#Matrix-Vector-Products:"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The matrix below gives the probabilities of moving from 1 health state to another in 1 year.  If the current health states for a group are:</p>
<ul>
<li>85% asymptomatic</li>
<li>10% symptomatic</li>
<li>5% AIDS</li>
<li>0% death</li>
</ul>
<p>what will be the % in each health state in 1 year?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html alt="floating point" style="width: 80%" file="/numerical_linear_algebra/images/markov_health.jpg" %}(Source: <a href="https://www.youtube.com/watch?v=0Il-y_WLTo4">Concepts of Markov Chains</a>)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Answer">Answer<a class="anchor-link" href="#Answer"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[ 0.765 ,  0.1525,  0.0645,  0.018 ]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Matrix-Matrix-Products">Matrix-Matrix Products<a class="anchor-link" href="#Matrix-Matrix-Products"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html alt="floating point" style="width: 100%" file="/numerical_linear_algebra/images/shop.png" %}(Source: <a href="https://www.mff.cuni.cz/veda/konference/wds/proc/pdf06/WDS06_106_m8_Ulrychova.pdf">Several Simple Real-world Applications of Linear Algebra Tools</a>)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Answer">Answer<a class="anchor-link" href="#Answer"> </a></h4>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[ 50. ,  49. ],
       [ 58.5,  61. ],
       [ 43.5,  43.5]])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Image-Data">Image Data<a class="anchor-link" href="#Image-Data"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Images can be represented by matrices.</p>
<p>{% include image.html alt="digit" style="width: 55%" file="/numerical_linear_algebra/images/digit.gif" %}
  (Source: <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721">Adam Geitgey</a>)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Convolution">Convolution<a class="anchor-link" href="#Convolution"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Convolutions</em> are the heart of convolutional neural networks (CNNs), a type of deep learning, responsible for the huge advances in image recognitionin the last few years.  They are now increasingly being used for speech as well, such as <a href="https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/">Facebook AI's results</a> for speech translation which are 9x faster than RNNs (the current most popular approach for speech translation).</p>
<p>Computers are now more accurate than people at classifying images.</p>
<p>{% include image.html alt="ImageNet" style="width: 80%" file="/numerical_linear_algebra/images/sportspredict.jpeg" %}
  (Source: <a href="http://karpathy.github.io/2014/07/03/feature-learning-escapades/">Andrej Karpathy</a>)</p>
<p>{% include image.html alt="ImageNet" style="width: 80%" file="/numerical_linear_algebra/images/InsideImagenet.png" %}
  (Source: <a href="https://blogs.nvidia.com/blog/2014/09/07/imagenet/">Nvidia</a>)</p>
<p>You can think of a convolution as a special kind of matrix product</p>
<p>The 3 images below are all from an excellent blog post written by a fast.ai student on <a href="https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c">CNNs from Different Viewpoints</a>:</p>
<p>A convolution applies a filter to each section of an image:
{% include image.html alt="CNNs" style="width: 40%" file="/numerical_linear_algebra/images/cnn1.png" %}</p>
<p>Neural Network Viewpoint:
{% include image.html alt="CNNs" style="width: 40%" file="/numerical_linear_algebra/images/cnn2.png" %}</p>
<p>Matrix Multiplication Viewpoint:
{% include image.html alt="CNNs" style="width: 80%" file="/numerical_linear_algebra/images/cnn3.png" %}</p>
<p>Let's see how convolutions can be used for <em>edge detection</em> in <a href="convolution-intro.ipynb">this notebook</a>(originally from the <a href="http://course.fast.ai/">fast.ai Deep Learning Course</a>)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Matrix-Decompositions">Matrix Decompositions<a class="anchor-link" href="#Matrix-Decompositions"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will be talking about Matrix Decompositions every day of this course, and will cover the below examples in future lessons:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><strong>Topic Modeling</strong> (NMF and SVD.  SVD uses QR)  A group of documents can be represented by a term-document matrix
{% include image.html alt="term-document matrix" style="width: 70%" file="/numerical_linear_algebra/images/document_term.png" %}
(source: <a href="http://player.slideplayer.com/15/4528582/#">Introduction to Information Retrieval</a>)
{% include image.html alt="NMF on documents" style="width: 70%" file="/numerical_linear_algebra/images/nmf_doc.png" %}
(source: <a href="http://perso.telecom-paristech.fr/~essid/teach/NMF_tutorial_ICME-2014.pdf">NMF Tutorial</a>)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><strong>Background removal</strong> (Truncated SVD)
<img src="/numerical_linear_algebra/images/surveillance3.png" alt="background removal"></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><strong>Removing noise</strong> (Robust PCA, which uses SVD)
<img src="/numerical_linear_algebra/images/faces_rpca.png" alt="background removal"></li>
</ul>
<p>This example comes from <a href="http://jeankossaifi.com/blog/rpca.html">Jean Kossaifi's blog</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><strong>Google's PageRank Algorithm</strong> (eigen decomposition)</li>
</ul>
<p>{% include image.html alt="PageRank" style="width: 70%" file="/numerical_linear_algebra/images/page_rank_graph.png" %}
  (source: <a href="http://computationalculture.net/article/what_is_in_pagerank">What is in PageRank?</a>)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>List of other decompositions and some applications <a href="https://sites.google.com/site/igorcarron2/matrixfactorizations">matrix factorization jungle</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Accuracy">Accuracy<a class="anchor-link" href="#Accuracy"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Floating-Point-Arithmetic">Floating Point Arithmetic<a class="anchor-link" href="#Floating-Point-Arithmetic"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To understand accuracy, we first need to look at <strong>how</strong> computers (which are finite and discrete) store numbers (which are infinite and continuous)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Exercise">Exercise<a class="anchor-link" href="#Exercise"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Take a moment to look at the function $f$ below.  Before you try running it, write on paper what the output would be of $x_1 = f(\frac{1}{10})$.  Now, (still on paper) plug that back into $f$ and calculate $x_2 = f(x_1)$.  Keep going for 10 iterations.</p>
<p>This example is taken from page 107 of <em>Numerical Methods</em>, by Greenbaum and Chartier.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Only after you've written down what you think the answer should be, run the code below:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">80</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>0.1
0.2
0.4
0.8
0.6000000000000001
0.20000000000000018
0.40000000000000036
0.8000000000000007
0.6000000000000014
0.20000000000000284
0.4000000000000057
0.8000000000000114
0.6000000000000227
0.20000000000004547
0.40000000000009095
0.8000000000001819
0.6000000000003638
0.2000000000007276
0.4000000000014552
0.8000000000029104
0.6000000000058208
0.20000000001164153
0.40000000002328306
0.8000000000465661
0.6000000000931323
0.20000000018626451
0.40000000037252903
0.8000000007450581
0.6000000014901161
0.20000000298023224
0.4000000059604645
0.800000011920929
0.6000000238418579
0.20000004768371582
0.40000009536743164
0.8000001907348633
0.6000003814697266
0.20000076293945312
0.40000152587890625
0.8000030517578125
0.600006103515625
0.20001220703125
0.4000244140625
0.800048828125
0.60009765625
0.2001953125
0.400390625
0.80078125
0.6015625
0.203125
0.40625
0.8125
0.625
0.25
0.5
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What went wrong?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Problem:-math-is-continuous-&amp;-infinite,-but-computers-are-discrete-&amp;-finite">Problem: math is continuous &amp; infinite, but computers are discrete &amp; finite<a class="anchor-link" href="#Problem:-math-is-continuous-&amp;-infinite,-but-computers-are-discrete-&amp;-finite"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Two Limitations of computer representations of numbers:</p>
<ol>
<li>they can't be arbitrarily large or small</li>
<li>there must be gaps between them</li>
</ol>
<p>The reason we need to care about accuracy, is because computers can't store infinitely accurate numbers.  It's possible to create calculations that give very wrong answers (particularly when repeating an operation many times, since each operation could multiply the error).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>How computers store numbers:</p>
<p>{% include image.html alt="floating point" style="width: 60%" file="/numerical_linear_algebra/images/fpa.png" %}</p>
<p>The <em>mantissa</em> can also be referred to as the <em>significand</em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>IEEE Double precision arithmetic:</p>
<ul>
<li>Numbers can be as large as $1.79 \times 10^{308}$ and as small as $2.23 \times 10^{-308}$.</li>
<li><p>The interval $[1,2]$ is represented by discrete subset: 
{% raw %}
$$1, \: 1+2^{-52}, \: 1+2 \times 2^{-52},\: 1+3 \times 2^{-52},\: \ldots, 2$$
{% endraw %}</p>
</li>
<li><p>The interval $[2,4]$ is represented:
{% raw %}
$$2, \: 2+2^{-51}, \: 2+2 \times 2^{-51},\: 2+3 \times 2^{-51},\: \ldots, 4$$
{% endraw %}</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Floats and doubles are not equidistant:</p>
<p>{% include image.html alt="floating point" style="width: 100%" file="/numerical_linear_algebra/images/fltscale-wh.png" %}
Source: <a href="http://www.volkerschatz.com/science/float.html">What you never wanted to know about floating point but will be forced to find out</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Machine Epsilon</strong></p>
<p>Half the distance between 1 and the next larger number. This can vary by computer.  IEEE standards for double precision specify $$ \varepsilon_{machine} = 2^{-53} \approx 1.11 \times 10^{-16}$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Two important properties of Floating Point Arithmetic</strong>:</p>
<ul>
<li><p>The difference between a real number $x$ and its closest floating point approximation $fl(x)$ is always smaller than $\varepsilon_{machine}$ in relative terms.  For some $\varepsilon$, where $\lvert \varepsilon \rvert \leq \varepsilon_{machine}$, $$fl(x)=x \cdot (1 + \varepsilon)$$</p>
</li>
<li><p>Where <em> is any operation ($+, -, \times, \div$), and $\circledast$ is its floating point analogue,
  $$ x \circledast y = (x </em> y)(1 + \varepsilon)$$
for some $\varepsilon$, where $\lvert \varepsilon \rvert \leq \varepsilon<em>{machine}$
That is, every operation of floating point arithmetic is exact up to a relative error of size at most $\varepsilon</em>{machine}$</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="History">History<a class="anchor-link" href="#History"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Floating point arithmetic may seem like a clear choice in hindsight, but there have been many, many ways of storing numbers:</p>
<ul>
<li>fixed-point arithmetic</li>
<li>logarithmic and semilogarithmic number systems</li>
<li>continued-fractions</li>
<li>rational numbers</li>
<li>possibly infinite strings of rational numbers</li>
<li>level-index number systems</li>
<li>fixed-slash and floating-slash number systems</li>
<li>2-adic numbers</li>
</ul>
<p>For references, see <a href="https://perso.ens-lyon.fr/jean-michel.muller/chapitre1.pdf">Chapter 1</a> (which is free) of the <a href="http://www.springer.com/gp/book/9780817647049">Handbook of Floating-Point Arithmetic</a>.  Yes, there is an entire 16 chapter book on floating point!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Timeline History of Floating Point Arithmetic:</p>
<ul>
<li>~1600 BC: Babylonian radix-60 system was earliest floating-point system (Donald Knuth).  Represented the significand of a radix-60 floating-point representation (if ratio of two numbers is a power of 60, represented the same)</li>
<li>1630 Slide rule.  Manipulate only significands (radix-10)</li>
<li>1914 Leonardo Torres y Quevedo described an electromechanical implementation of Babbage's Analytical Engine with Floating Point Arithmetic.</li>
<li>1941 First real, modern implementation.  Konrad Zuse's Z3 computer.  Used radix-2, with 14 bit significand, 7 bit exponents, and 1 sign bit.</li>
<li>1985 IEEE 754-1985 Standard for Binary Floating-Point Arithmetic released.  Has increased accuracy, reliability, and portability.  <a href="https://people.eecs.berkeley.edu/~wkahan/">William Kahan</a> played leading role.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>"Many different ways of approximating real numbers on computers have been introduced.. And yet, floating-point arithmetic is <strong>by far the most widely used</strong> way of representing real numbers in modern computers. Simulating an infinite, continuous set (the real numbers) with a finite set (the “machine numbers”) is not a straightforward task: <strong>clever compromises must be found between, speed, accuracy, dynamic range, ease of use and implementation, and memory</strong>. It appears that floating-point arithmetic, with adequately chosen parameters (radix, precision, extremal exponents, etc.), is a very good compromise for most numerical applications."</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Although a radix value of 2 (binary) seems like the pretty clear winner now for computers, a variety of other radix values have been used at various point:</p>
<ul>
<li>radix-8 used by early machines PDP-10, Burroughs 570 and 6700</li>
<li>radix-16 IBM 360</li>
<li>radix-10 financial calculations, pocket calculators, Maple</li>
<li>radix-3 Russian SETUN computer (1958).  Benefits: minimizes beta x p (symbols x digits), for a fixed largest representable number beta^p - 1.  Rounding = truncation</li>
<li>radix-2 most common.  Reasons: easy to implement.  Studies have shown (with implicit leading bit) this gives better worst-case or average accuracy than all other radices.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Conditioning-and-Stability">Conditioning and Stability<a class="anchor-link" href="#Conditioning-and-Stability"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since we can not represent numbers exactly on a computer (due to the finiteness of our storage, and the gaps between numbers in floating point architecture), it becomes important to know <em>how small perturbations in the input to a problem impact the output</em>.</p>
<p><strong>"A stable algorithm gives nearly the right answer to nearly the right question."</strong> --Trefethen</p>
<p><strong>Conditioning</strong>: perturbation behavior of a mathematical problem (e.g. least squares)</p>
<p><strong>Stability</strong>: perturbation behavior of an algorithm used to solve that problem on a computer (e.g. least squares algorithms, householder, back substitution, gaussian elimination)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Example: Eigenvalues of a Matrix</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.linalg</span> <span class="k">as</span> <span class="nn">la</span> 

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[    1.  1000.]
 [    0.     1.]]
[[  1.00000000e+00   1.00000000e+03]
 [  1.00000000e-03   1.00000000e+00]]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wA</span><span class="p">,</span> <span class="n">vrA</span> <span class="o">=</span> <span class="n">la</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">wB</span><span class="p">,</span> <span class="n">vrB</span> <span class="o">=</span> <span class="n">la</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>

<span class="n">wA</span><span class="p">,</span> <span class="n">wB</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(array([ 1.+0.j,  1.+0.j]), array([ 2.+0.j,  0.+0.j]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Reminder: Two properties of Floating Point Arithmetic</strong></p>
<ul>
<li><p>The difference between a real number $x$ and its closest floating point approximation $fl(x)$ is always smaller than $\varepsilon_{machine}$ in relative terms.</p>
</li>
<li><p>Every operation $+, -, \times, \div$ of floating point arithmetic is exact up to a relative error of size at most $\varepsilon_{machine}$</p>
</li>
</ul>
<p>Examples we'll see:</p>
<ul>
<li>Classical vs Modified Gram-Schmidt accuracy</li>
<li>Gram-Schmidt vs. Householder (2 different ways of computing QR factorization), how orthogonal the answer is</li>
<li>Condition of a system of equations</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Approximation-accuracy">Approximation accuracy<a class="anchor-link" href="#Approximation-accuracy"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's rare that we need to do highly accurate matrix computations at scale. In fact, often we're doing some kind of machine learning, and less accurate approaches can prevent overfitting.</p>
<p>In many cases, are input data may not be that precise, so it doesn't make sense to seek a higher level of accuracy with our calculations then the input data allows.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Random-Data-Structures">Random Data Structures<a class="anchor-link" href="#Random-Data-Structures"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Bloom Filters</li>
<li>HyperLogLog</li>
<li>Locality-Sensitive Hashing</li>
<li>Skip Lists</li>
<li>Count-min sketch</li>
<li>Min-hash</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Example</strong>: A <strong>bloom filter</strong> allows searching for set membership with 1% false positives, using &lt;10 bits per element. This often represents reductions in memory use of thousands of times.</p>
<p>{% include image.html alt="Bloom Filters" style="width: 60%" file="/numerical_linear_algebra/images/bloom_filter.png" %}</p>
<p>The false positives can be easily handled by having a second (exact) stage check all returned items - for rare items this can be very effective. For instance, many web browsers use a bloom filter to create a set of blocked pages (e.g. pages with viruses), since blocked web pages are only a small fraction of the whole web. A false positive can be handled here by taking anything returned by the bloom filter and checking against a web service with the full exact list.  (See this <a href="https://llimllib.github.io/bloomfilter-tutorial/">bloom filter tutorial</a> for more details).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Random-Algorithms">Random Algorithms<a class="anchor-link" href="#Random-Algorithms"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Karger's algorithm (minimum cut of a graph)</li>
<li>Randomized regression</li>
<li>Monte Carlo simulation</li>
<li>Randomized LU decomposition (Gaussian Elimination)</li>
<li>Randomized SVD</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we accept some decrease in accuracy, then we can often increase speed by orders of magnitude (and/or decrease memory use) by using approximate algorithms. These algorithms typically give a correct answer with some probability. By rerunning the algorithm multiple times you can generally increase that probability multiplicatively!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Expensive-Errors">Expensive Errors<a class="anchor-link" href="#Expensive-Errors"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>The below examples are from Greenbaum &amp; Chartier.</em></p>
<p>European Space Agency spent 10 years and $7 billion on the Ariane 5 Rocket.</p>
<p>What can happen when you try to fit a 64 bit number into a 16 bit space (integer overflow):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">YouTubeVideo</span><span class="p">(</span><span class="s2">&quot;PK_yguLapgA&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">

        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/PK_yguLapgA"
            frameborder="0"
            allowfullscreen
        ></iframe>
        
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is a floating point error that cost Intel $475 million:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="http://www.nytimes.com/1994/11/24/business/company-news-flaw-undermines-accuracy-of-pentium-chips.html">1994 NYTimes article about Intel Pentium Error</a>
<img src="/numerical_linear_algebra/images/pentium_nytimes.png" alt="article"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Resources</strong>: See Lecture 13 of Trefethen &amp; Bau and Chapter 5 of Greenbaum &amp; Chartier for more on Floating Point Arithmetic</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Memory-Use">Memory Use<a class="anchor-link" href="#Memory-Use"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sparse-vs-Dense">Sparse vs Dense<a class="anchor-link" href="#Sparse-vs-Dense"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above we covered how <em>numbers</em> are stored, now let's talk about how <em>matrices</em> are stored. A key way to save memory (and computation) is not to store all of your matrix. Instead, just store the non-zero elements. This is called <strong>sparse</strong> storage, and it is well suited to sparse matrices, that is, matrices where most elements are zero.</p>
<p>{% include image.html alt="floating point" style="width: 50%" file="/numerical_linear_algebra/images/sparse.png" %}</p>
<p>Here is an example of the matrix from a finite element problem, which shows up in engineering (for instance, when modeling the air-flow around a plane). In this example, the non-zero elements are black and the zero elements are white:
{% include image.html alt="floating point" style="width: 50%" file="/numerical_linear_algebra/images/Finite_element_sparse_matrix.png" %}
<a href="https://commons.wikimedia.org/w/index.php?curid=2245335">Source</a></p>
<p>There are also special types of structured matrix, such as diagonal, tri-diagonal, hessenberg, and triangular, which each display particular patterns of sparsity, which can be leveraged to reduce memory and computation.</p>
<p>The opposite of a sparse matrix is a <strong>dense</strong> matrix, along with dense storage, which simply refers to a matrix containing mostly non-zeros, in which every element is stored explicitly.  Since sparse matrices are helpful and common, numerical linear algebra focuses on maintaining sparsity through as many operations in a computation as possible.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Speed">Speed<a class="anchor-link" href="#Speed"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Speed differences come from a number of areas, particularly:</p>
<ul>
<li>Computational complexity</li>
<li>Vectorization</li>
<li>Scaling to multiple cores and nodes</li>
<li>Locality</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Computational-complexity">Computational complexity<a class="anchor-link" href="#Computational-complexity"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Algorithms are generally expressed in terms of computation complexity with respect to the number of rows and number of columns in the matrix. E.g. you may find an algorithm described as $\mathcal{O(n^2m)}$.</p>
<p>Computational complexity and $\mathcal{O}$ notation frequently show up in tech interviews, so it's a good idea to practice them.
You can learn about the concepts and practice at these sites:</p>
<ul>
<li><a href="https://www.interviewcake.com/article/java/big-o-notation-time-and-space-complexity">Interview Cake</a></li>
<li><a href="https://www.codecademy.com/courses/big-o/0/3">Codecademy</a></li>
<li><a href="https://www.hackerrank.com/contests/30-days-of-code/challenges/day-25-running-time">HackerRank</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Vectorization">Vectorization<a class="anchor-link" href="#Vectorization"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Modern CPUs and GPUs can apply an operation to multiple elements at once on a single core. For instance, take the exponent of 4 floats in a vector in a single step. This is called SIMD. You will not be explicitly writing SIMD code (which tends to require assembly language or special C "intrinsics"), but instead will use vectorized operations in libraries like numpy, which in turn rely on specially tuned vectorized low level linear algebra APIs (in particular, BLAS, and LAPACK).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Matrix-Computation-Packages:-BLAS-and-LAPACK">Matrix Computation Packages: BLAS and LAPACK<a class="anchor-link" href="#Matrix-Computation-Packages:-BLAS-and-LAPACK"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="http://www.netlib.org/blas/">BLAS (Basic Linear Algebra Subprograms)</a>: specification for low-level matrix and vector arithmetic operations. These are the standard building blocks for performing basic vector and matrix operations.  BLAS originated as a Fortran library in 1979.  Examples of BLAS libraries include: AMD Core Math Library (ACML), ATLAS, Intel Math Kernel Library (MKL), and OpenBLAS.</p>
<p><a href="http://www.netlib.org/lapack/">LAPACK</a> is written in Fortran, provides routines for solving systems of linear equations, eigenvalue problems, and singular value problems.  Matrix factorizations (LU, Cholesky, QR, SVD, Schur).  Dense and banded matrices are handled, but not general sparse matrices.  Real and complex, single and double precision.</p>
<p>1970s and 1980s: EISPACK (eigenvalue routines) and LINPACK (linear equations and linear least-squares routines) libraries</p>
<p><strong>LAPACK original goal</strong>: make LINAPCK and EISPACK run efficiently on shared-memory vector and parallel processors and exploit cache on modern cache-based architectures (initially released in 1992).  EISPACK and LINPACK ignore multi-layered memory hierarchies and spend too much time moving data around.</p>
<p>LAPACK uses highly optimized block operations implementations (which much be implemented on each machine) LAPACK written so as much of the computation as possible is performed by BLAS.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Locality">Locality<a class="anchor-link" href="#Locality"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using slower ways to access data (e.g. over the internet) can be up to a billion times slower than faster ways (e.g. from a register). But there's much less fast storage than slow storage. So once we have data in fast storage, we want to do any computation required at that time, rather than having to load it multiple times each time we need it. In addition, for most types of storage its much faster to access data items that are stored next to each other, so we should try to always use any data stored nearby that we know we'll need soon. These two issues are known as locality.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Speed-of-different-types-of-memory">Speed of different types of memory<a class="anchor-link" href="#Speed-of-different-types-of-memory"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are some <em>numbers everyone should know</em> (from the legendary <a href="http://static.googleusercontent.com/media/research.google.com/en/us/people/jeff/stanford-295-talk.pdf">Jeff Dean</a>):</p>
<ul>
<li>L1 cache reference 0.5 ns</li>
<li>L2 cache reference 7 ns</li>
<li>Main memory reference/RAM 100 ns</li>
<li>Send 2K bytes over 1 Gbps network 20,000 ns</li>
<li>Read 1 MB sequentially from memory 250,000 ns</li>
<li>Round trip within same datacenter 500,000 ns</li>
<li>Disk seek 10,000,000 ns</li>
<li>Read 1 MB sequentially from network 10,000,000 ns</li>
<li>Read 1 MB sequentially from disk 30,000,000 ns</li>
<li>Send packet CA-&gt;Netherlands-&gt;CA 150,000,000 ns</li>
</ul>
<p>And here is an updated, interactive <a href="https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html">version</a>, which includes a timeline of how these numbers have changed.</p>
<p><strong>Key take-away</strong>: Each successive memory type is (at least) an order of magnitude worse than the one before it.  Disk seeks are <strong>very slow</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This video has a great example of showing several ways you could compute the blur of a photo, with various trade-offs. Don't worry about the C code that appears, just focus on the red and green moving pictures of matrix computation.</p>
<p>Although the video is about a new language called Halide, it is a good illustration the issues it raises are universal.  Watch minutes 1-13:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="n">YouTubeVideo</span><span class="p">(</span><span class="s2">&quot;3uiEyEKji0M&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">

        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/3uiEyEKji0M"
            frameborder="0"
            allowfullscreen
        ></iframe>
        
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Locality is hard.  Potential trade-offs:</p>
<ul>
<li>redundant computation to save memory bandwidth</li>
<li>sacrificing parallelism to get better reuse</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Temporaries">Temporaries<a class="anchor-link" href="#Temporaries"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The issue of "temporaries" occurs when the result of a calculation is stored in a temporary variable in RAM, and then that variable is loaded to do another calculation on it. This is many orders of magnitude slower than simply keeping the data in cache or registers and doing all necessary computations before storing the final result in RAM. This is particularly an issue for us since numpy generally creates temporaries for every single operation or function it does. E.g. $a=b\cdot c^2+ln(d)$ will create four temporaries (since there are four operations and functions).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Scaling-to-multiple-cores-and-nodes">Scaling to multiple cores and nodes<a class="anchor-link" href="#Scaling-to-multiple-cores-and-nodes"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have a separate section for scalability, but it’s worth noting that this is also important for speed - if we can't scale across all the computing resources we have, we'll be stuck with slower computation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Scalability-/-parallelization">Scalability / parallelization<a class="anchor-link" href="#Scalability-/-parallelization"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Often we'll find that we have more data than we have memory to handle, or time to compute. In such a case we would like to be able to scale our algorithm across <a href="http://www.makeuseof.com/tag/processor-core-makeuseof-explains-2/">multiple cores</a> (within one computer) or nodes (i.e. multiple computers on a network). We will not be tackling multi-node scaling in this course, although we will look at scaling across multiple cores (called parallelization). In general, scalable algorithms are those where the input can be broken up into smaller pieces, each of which are handled by a different core/computer, and then are put back together at the end.</p>

</div>
</div>
</div>
</div>
 

